# Python End-to-End ETL Pipeline Project

## ğŸ“Œ Project Overview
This project implements a complete **End-to-End ETL (Extract, Transform, Load) Pipeline** using **Python**, simulating a real-world **Data Engineering** scenario.

The pipeline extracts data from multiple heterogeneous sources (API, MySQL Database, and Local Data Lake), applies data quality checks and business transformations, builds a **dimensional data model (Star Schema)**, and generates **business insights** through data visualization.

The project follows best practices in data engineering such as modular design, staging layers, metadata enrichment, and clear separation of responsibilities between pipeline phases.

---
ğŸ—‚ï¸ Project Structure
project-root/
â”‚
â”œâ”€â”€ DB_Connection/        # Database connection scripts (MySQL)
â”‚
â”œâ”€â”€ DataLake/             # Raw data storage (CSV / JSON files)
â”‚
â”œâ”€â”€ extracted/            # Extracted datasets from API, DB, and Data Lake
â”‚
â”œâ”€â”€ staging_1/            # First staging layer (cleaned & validated data)
â”‚
â”œâ”€â”€ staging_2/            # Second staging layer (transformed & enriched data)
â”‚
â”œâ”€â”€ Information_Mart/     # Final processed data ready for analytics
â”‚
â”œâ”€â”€ Visualizations/       # Generated charts and reports
â”‚
â”œâ”€â”€ schema_model.db       # Database schema (dimension & fact tables)
â”‚
â”œâ”€â”€ data_mart.db          # Final data mart database
â”‚
â”œâ”€â”€ Schema_Diagram.png    # ER / Star Schema diagram
â”‚
â”œâ”€â”€ requirements.txt      # Python dependencies
â”‚
â”œâ”€â”€ Extraction.py         # Data extraction logic (API, MySQL, Data Lake)
â”‚
â”œâ”€â”€ Transformation.py    # Data cleaning and transformation logic
â”‚
â”œâ”€â”€ Modeling.py           # Data aggregation and dimensional modeling
â”‚
â”œâ”€â”€ Quality_check.py     # Data quality validation checks
â”‚
â”œâ”€â”€ Visualization.py     # Charts and business insights generation
â”‚
â””â”€â”€ main.py               # Main ETL pipeline execution



## ğŸ¯ Project Objectives
- Extract data from different sources using Python
- Perform data quality checks and cleansing
- Apply business transformations and enrichment
- Design and build a dimensional data model for analytics
- Generate BI insights using charts and visualizations

---

## ğŸ“¥ Data Sources

### 1ï¸âƒ£ API
- **OpenExchangeRates API**
- Used to retrieve currency exchange rates for price conversion

### 2ï¸âƒ£ Relational Database (MySQL)
- Extracted using **mysql.connector**
- Tables:
  - `orders`
  - `order_items`

### 3ï¸âƒ£ Data Lake (Local File System)
CSV datasets simulating a data lake environment:
- `customers.csv`
- `products.csv`
- `brands.csv`
- `categories.csv`
- `stores.csv`
- `staffs.csv`
- `stocks.csv`

---

## ğŸ”„ ETL Pipeline Phases

### 1ï¸âƒ£ Extraction
- Data extracted from:
  - REST API
  - MySQL database using explicit SQL queries
  - Local CSV files
- Metadata added:
  - `extraction_timestamp`
  - `data_source`
- Raw data stored in structured folders

---

### 2ï¸âƒ£ Data Quality Checks
- Null value validation
- Duplicate detection and removal
- Data type and value range validation
- Cleaned data stored in **staging_1**

---

### 3ï¸âƒ£ Transformation
Key transformations include:
- Currency conversion using latest exchange rates
- Delivery performance metrics:
  - Late delivery flag
  - Delivery latency (days)
- Customer locality classification
- Lookup table resolution for order statuses

Transformed data stored in **staging_2**

---

### 4ï¸âƒ£ Data Modeling
- Designed a **Star Schema** for analytics
- Built:
  - Fact tables for transactional data
  - Dimension tables for descriptive attributes
- Integrated orders, order items, and product data
- Final analytical datasets stored in **Information Mart**

---

### 5ï¸âƒ£ Reporting & Visualization
- Generated business insights using:
  - Time-series analysis (Sales Over Time)
  - Top-N analysis (Best Selling Products)
  - Distribution analysis (Customer Segmentation)
- Visualizations created using:
  - `matplotlib`
  - `seaborn`

---

## ğŸ—‚ï¸ Project Structure

